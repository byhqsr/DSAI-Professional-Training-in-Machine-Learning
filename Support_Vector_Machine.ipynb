{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_Vector_Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKqvfP4jhJRkamPxOEHVwX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byhqsr/DSAI-Professional-Training-in-Machine-Learning/blob/main/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMO7MxKSk1N"
      },
      "source": [
        "As we just did with logistic regression, in this exercise, we are going to perform classification on the penguin dataset using Support Vector Machines.\n",
        "\n",
        "Data Scrubbing:\n",
        "*   Removing rows with missing values\n",
        "*   One-hot encoding for island and sex\n",
        "*   Standardization using StandardScaler for all independent variables\n",
        "\n",
        "Independent Variables:\n",
        "*   bill_length_mm\n",
        "*   bill_depth_mm\n",
        "*   flipper_length_mm\n",
        "*   day body_mass_g\n",
        "*   island\n",
        "*   sex\n",
        "\n",
        "Dependent Variable:\n",
        "*   species\n",
        "\n",
        "Evaluation:\n",
        "*   Confusion matrix\n",
        "*   Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dN-ZlM8Sb0K"
      },
      "source": [
        "# 1) Import the following Python libraries: A) pandas B) train_test_split from Scikit-learn C) StandardScaler from Scikit-learn D) SVC from Scikit-learn E) confusion_matrix and classification_report from Scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmXk9I5fTfbu"
      },
      "source": [
        "# 2) Import dataset from the web: https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCDXRDSFTlsi"
      },
      "source": [
        "# 3) Delete rows containing missing values\n",
        "df.dropna(axis = 0, how = 'any', thresh = None, subset = None, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWUpuyPDTtiw"
      },
      "source": [
        "# 4) Convert non-numeric variables using one-hot encoding. These variables include sex and island.\n",
        "df = pd.get_dummies(df, columns=['sex', 'island'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M276KU48T13W"
      },
      "source": [
        "# 5) Standardize the independent variables using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df.drop('species',axis=1))\n",
        "scaled_features = scaler.transform(df.drop('species',axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EgQeCO_UPTs"
      },
      "source": [
        "# 6) Assign the X and y variables\n",
        "X = scaled_features\n",
        "y = df['species']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iNTMMCTUg6C"
      },
      "source": [
        "# 7) Shuffle the dataset and split the data into test/train sets (70/30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UqsG0HUulY"
      },
      "source": [
        "# 8) Assign the classification version of Support Vectors Machine (SVC) as the model's algorithm\n",
        "model = SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZvgsILU4ys"
      },
      "source": [
        "# 9) Link model to X and y variables using the fit function\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_4Kv5X2VBa5"
      },
      "source": [
        "# 10) Run algorithm on test data to make predictions\n",
        "model_test = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYZ-8G4UVLNY"
      },
      "source": [
        "# 11) Evaluate predictions by comparing the model's predictions and the actual outcome of the test data using a confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, model_test)) \n",
        "print(classification_report(y_test, model_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOSLBN1VVW9a"
      },
      "source": [
        "# 12) Make a prediction with the model using a sample data point (called 'penguin') and the predict function\n",
        "# Data point to predict\n",
        "penguin = [\n",
        "\t39, #bill_length_mm\n",
        "\t18.5, #bill_depth_mm\n",
        "\t180, #flipper_length_mm \n",
        "\t3750, #body_mass_g\n",
        "\t0, #island_Biscoe    \n",
        "\t0, #island_Dream\n",
        "\t1, #island_Torgersen    \n",
        "\t1, #sex_Male\n",
        "\t0, #sex_Female\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W4_A7cfWrrR"
      },
      "source": [
        "# Make prediction\n",
        "new_penguin = model.predict([penguin])\n",
        "new_penguin"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}