{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_Vector_Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKJ5bFyp/dhjRL3Kua3Xm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byhqsr/DSAI-Professional-Training-in-Machine-Learning/blob/main/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMO7MxKSk1N"
      },
      "source": [
        "As we just did with logistic regression, in this exercise, we are going to perform classification on the penguin dataset using Support Vector Machines.\n",
        "\n",
        "Data Scrubbing:\n",
        "*   Removing rows with missing values\n",
        "*   One-hot encoding for island and sex\n",
        "*   Standardization using StandardScaler for all independent variables\n",
        "\n",
        "Independent Variables:\n",
        "*   bill_length_mm\n",
        "*   bill_depth_mm\n",
        "*   flipper_length_mm\n",
        "*   day body_mass_g\n",
        "*   island\n",
        "*   sex\n",
        "\n",
        "Dependent Variable:\n",
        "*   species\n",
        "\n",
        "Evaluation:\n",
        "*   Confusion matrix\n",
        "*   Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dN-ZlM8Sb0K"
      },
      "source": [
        "# 1) Import the following Python libraries: A) pandas B) train_test_split from Scikit-learn C) StandardScaler from Scikit-learn D) SVC from Scikit-learn E) confusion_matrix and classification_report from Scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmXk9I5fTfbu"
      },
      "source": [
        "# 2) Import dataset from the web: https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCDXRDSFTlsi"
      },
      "source": [
        "# 3) Delete rows containing missing values\n",
        "df.dropna(axis = 0, how = 'any', thresh = None, subset = None, inplace = True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWUpuyPDTtiw"
      },
      "source": [
        "# 4) Convert non-numeric variables using one-hot encoding. These variables include sex and island.\n",
        "df = pd.get_dummies(df, columns=['sex', 'island'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M276KU48T13W"
      },
      "source": [
        "# 5) Standardize the independent variables using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df.drop('species',axis=1))\n",
        "scaled_features = scaler.transform(df.drop('species',axis=1))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EgQeCO_UPTs"
      },
      "source": [
        "# 6) Assign the X and y variables\n",
        "X = scaled_features\n",
        "y = df['species']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iNTMMCTUg6C"
      },
      "source": [
        "# 7) Shuffle the dataset and split the data into test/train sets (70/30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UqsG0HUulY"
      },
      "source": [
        "# 8) Assign the classification version of Support Vectors Machine (SVC) as the model's algorithm\n",
        "model = SVC()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FZvgsILU4ys",
        "outputId": "aa538cae-4f32-4939-9608-ee3594d6c33b"
      },
      "source": [
        "# 9) Link model to X and y variables using the fit function\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_4Kv5X2VBa5"
      },
      "source": [
        "# 10) Run algorithm on test data to make predictions\n",
        "model_test = model.predict(X_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYZ-8G4UVLNY",
        "outputId": "b22d0162-f044-4bda-a3cb-0699fd072e75"
      },
      "source": [
        "# 11) Evaluate predictions by comparing the model's predictions and the actual outcome of the test data using a confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, model_test)) \n",
        "print(classification_report(y_test, model_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[44  0  0]\n",
            " [ 0 20  0]\n",
            " [ 0  0 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      1.00      1.00        44\n",
            "   Chinstrap       1.00      1.00      1.00        20\n",
            "      Gentoo       1.00      1.00      1.00        36\n",
            "\n",
            "    accuracy                           1.00       100\n",
            "   macro avg       1.00      1.00      1.00       100\n",
            "weighted avg       1.00      1.00      1.00       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOSLBN1VVW9a",
        "outputId": "d63c723b-257d-4411-aaba-6624349d38e5"
      },
      "source": [
        "# 12) Make a prediction with the model using a sample data point (called 'penguin') and the predict function\n",
        "# Data point to predict\n",
        "penguin = [\n",
        "\t39, #bill_length_mm\n",
        "\t18.5, #bill_depth_mm\n",
        "\t180, #flipper_length_mm \n",
        "\t3750, #body_mass_g\n",
        "\t0, #island_Biscoe    \n",
        "\t0, #island_Dream\n",
        "\t1, #island_Torgersen    \n",
        "\t1, #sex_Male\n",
        "\t0, #sex_Female\n",
        "]\n",
        "\n",
        "# Make prediction\n",
        "new_penguin = model.predict([penguin])\n",
        "new_penguin"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Adelie'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}