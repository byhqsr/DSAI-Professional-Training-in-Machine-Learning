{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWKImyS7/UjEtRWfzDqTiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byhqsr/DSAI-Professional-Training-in-Machine-Learning/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNoX3x4yIZ9c"
      },
      "source": [
        "In this lesson, we're going to use logistic regression to predict the species of penguin.\n",
        "\n",
        "To recap, logistic regression is a classification technique for predicting a qualitative outcome, such as \"positive\" or \"negative\" in response to a COVID test, which is different from linear regression (where the outcome is numerical/quantitative).\n",
        "\n",
        "The dataset chosen for this example is the penguins' dataset available with Seaborn (https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv). This dataset contains information about penguins including their bill and flipper dimensions as well as mass, species, and the island they inhabit. Note that this is a snippet of the dataset and the full dataset has 344 rows (or 344 penguins).\n",
        "\n",
        "Data Scrubbing:\n",
        "*   Removing rows with missing values\n",
        "*   One-hot encoding for island and sex\n",
        "\n",
        "Independent Variables:\n",
        "*   bill_length_mm\n",
        "*   bill_depth_mm\n",
        "*   flipper_length_mm\n",
        "*   day body_mass_g\n",
        "*   island\n",
        "*   sex\n",
        "\n",
        "Dependent Variable:\n",
        "*   species\n",
        "\n",
        "Evaluation:\n",
        "*   Confusion matrix\n",
        "*   Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wicrVdsDAp7r"
      },
      "source": [
        "# 1) Import the following Python libraries: A) pandas B) train_test_split from Scikit-learn C) LogisticRegression from Scikit-learn D) confusion_matrix and classification_report from Scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxz-DJN5B-2w"
      },
      "source": [
        "# 2) Import dataset from the web: https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKyK6ahCNzw"
      },
      "source": [
        "# 3) Delete rows with missing values\n",
        "df.dropna(axis = 0, how = 'any', thresh = None, subset = None, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-mCX-elCcu1"
      },
      "source": [
        "# 4) Convert non-numeric variables using one-hot encoding. These variables are: island and sex\n",
        "df = pd.get_dummies(df, columns=['island', 'sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8lzy7sLClkn"
      },
      "source": [
        "# 5) Assign the X and y variables\n",
        "X = df.drop('species',axis=1)\n",
        "y = df['species']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4hkT_VxCtUH"
      },
      "source": [
        "# 6) Shuffle the dataset and split the data into test/train sets (70/30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnggYSMBC2p0"
      },
      "source": [
        "# 7) Assign LogisticRegression as the model's algorithm\n",
        "model = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Kgmu8TC-bf"
      },
      "source": [
        "# 8) Link model to X and y variables using the fit function\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMgXvYj4DL-z"
      },
      "source": [
        "# 9) Run algorithm on test data to make predictions\n",
        "model_test = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNsYTflLDYGG"
      },
      "source": [
        "# 10) Evaluate predictions by comparing the model's predictions with the actual outcome of the test data using a confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, model_test)) \n",
        "print(classification_report(y_test, model_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7jlwGSsDgJ4"
      },
      "source": [
        "# 11) Make a prediction with the model using a sample data point (called 'penguin') and the predict function\n",
        "penguin = [\n",
        "\t39, #bill_length_mm\n",
        "\t18.5, #bill_depth_mm\n",
        "\t180, #flipper_length_mm \n",
        "\t3750, #body_mass_g\n",
        "\t0, #island_Biscoe    \n",
        "\t0, #island_Dream\n",
        "\t1, #island_Torgersen    \n",
        "\t1, #sex_Male\n",
        "\t0, #sex_Female\n",
        "]\n",
        "\n",
        "# Make prediction\n",
        "new_penguin = model.predict([penguin])\n",
        "new_penguin"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}